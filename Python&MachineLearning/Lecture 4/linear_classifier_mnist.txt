import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data

from random import randint
import matplotlib.pyplot as plt

mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)

r = randint(0, mnist.test.num_examples - 1)
plt.imshow(mnist.test.images[r:r+1].reshape(28, 28), cmap='Greys', interpolation='nearest')
plt.show()
print(mnist.test.labels[r:r+1])


x = tf.placeholder(tf.float32, [None, 784])
y = tf.placeholder(tf.float32, [None, 10])

W = tf.Variable(tf.random_normal(shape=(784, 10), mean=0.0, stddev=1.0, dtype=tf.float32))
b = tf.Variable(tf.random_normal(shape=(10,)))

scores = tf.matmul(x, W) + b
prob = tf.nn.softmax(scores)

cross_entrophy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = scores))

train = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entrophy_loss)


init = tf.global_variables_initializer()

with tf.Session() as sess:

    sess.run(init)

    for epoch in range(30):
        avg_loss = 0.
        for step in range(mnist.train.num_examples // 100):
            batch_x, batch_y = mnist.train.next_batch(100)
            loss, _ = sess.run([cross_entrophy_loss, train], feed_dict={x:batch_x, y:batch_y})
            avg_loss += loss / (mnist.train.num_examples // 100)
        print("loss: ", avg_loss)


    r = randint(0, mnist.test.num_examples - 1)
    plt.imshow(mnist.test.images[r:r+1].reshape(28, 28), cmap='Greys', interpolation='nearest')
    plt.show()
    print("Prediction: ", sess.run(tf.argmax(scores, 1), feed_dict={x: mnist.test.images[r:r+1]}))
